{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring User Clustering Representations to identify user structure\n",
    "\n",
    "This exploration's goal is to grasp not necessarily how the provided user-rating-data is distributed (average number of ratings per user, number of users with more than 10 ratings etc.) but how expressive these are concerning actual user profiling.\n",
    "\n",
    "This is done by preparing a _user-food-matrix_ and applying dimensionality reduction on them to see how well these separate in low-dimensional (2D) space.\n",
    "\n",
    "Subsequently, a _user-user-matrix_ is constructed to - by contrast to the (very sparse) user-food-matrix - compare user profile similarities (kind of 2nd-moment-comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from util.cloud_connection import bucket_connection\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# constants\n",
    "RATING_CSV = 'data/rating_normalized.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv(RATING_CSV)\n",
    "df = bucket_connection.get_meals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we merge the normalizer (primary) title with the rating matrix - essentially looking up `m_id` - to see what meals the ratings actually belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = df_ratings.assign(\n",
    "    title_prim=[df.loc[(df['m_id']==m_id),'title_prim'].to_string(index=False) \n",
    "                for m_id in df_ratings.loc[:,'m_id']])\n",
    "\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate (sparse) user-item-matrix\n",
    "\n",
    "Now build up a (pretty _sparse_ ...) user-(primary)meal-matrix across all (primary) meal titles where for each user its ratings are entered at the corresponding position.\n",
    "\n",
    "Note: Users may have rated food at different times where the food with primary title is the same. These ratings are averaged (`np.mean`) and entered as a single rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_user_item = df_ratings.pivot_table(index=\"user\",\n",
    "                                      columns=\"title_prim\",\n",
    "                                      values=\"rating\",\n",
    "                                      aggfunc=np.mean).fillna(0)\n",
    "\n",
    "\n",
    "print(\"Shape: {}, Size: {}\".format(df_user_item.shape, df_user_item.size))\n",
    "print(\"Non-zero entries: {}\".format(np.count_nonzero(df_user_item)))\n",
    "print(\"... making up {}% of entries\".format(round((np.count_nonzero(df_user_item)/df_user_item.size)*100, 4)))\n",
    "\n",
    "df_user_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute user similarities & generate user-user-matrix\n",
    "\n",
    "... and subsequently breifly evaluate each generated matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "user_user_mats = []\n",
    "for metric in ['correlation', 'cosine', 'dice', 'jaccard']:\n",
    "    df = pd.DataFrame(1 - pairwise_distances(df_user_item, metric=metric))    \n",
    "    user_user_mats.append({'metric':metric,'df':df})\n",
    "    \n",
    "def print_stats(mat):\n",
    "    metric, df = mat['metric'], mat['df']\n",
    "    print(\"--> Simi-fct.: {}\".format(metric))\n",
    "    print(\"Shape: {}, Size: {}\".format(df.shape, df.size))\n",
    "    print(df.iloc[:7,:7].round(4))\n",
    "    print(\"\\n> Row-sums: {}\".format(list(df.sum().round(2))))\n",
    "    print(\"> Total-sum:\\t{}\".format(round(df.sum().sum(), 4)))\n",
    "    entries_zero = df.size - np.count_nonzero(df)\n",
    "    print(\"\\n> Zero-Entries:\\t{}%\\t({} entries)\".format(round(entries_zero/df.size*100, 2), entries_zero))\n",
    "    entries_pos = len(np.where(df > 0.0)[0])\n",
    "    print(\"> Positive:\\t{}%\\t({} entries)\".format(round(entries_pos/df.size*100, 2), entries_pos))\n",
    "    entries_neg = len(np.where(df < 0.0)[0])\n",
    "    print(\"> Negative:\\t{}%\\t({} entries)\".format(round(entries_neg/df.size*100, 2), entries_neg))\n",
    "    entries_greater_dot5 = len(np.where(df > 0.5)[0])\n",
    "    print(\"> Greater 0.5:\\t{}%\\t({} entries)\".format(round(entries_greater_dot5/df.size*100, 2), entries_greater_dot5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(user_user_mats[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Pearson) Correlation - Evaluation\n",
    "\n",
    "Only 8 valus bigger then 0.5 (when substracting the 53 values of 1.0 o the diagonal of the matrix).\n",
    "\n",
    "With the (Pearson) correlation coefficient we get a dense matrix, but it has a lot of negative values. We need to check how those affect the further recommendation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(user_user_mats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine - Evaluation\n",
    "\n",
    "We have one user whose sum of cosine similarities for all other users including himself is 1. This means he has no similarity with any other user.\n",
    "\n",
    "The user item matrix is 6.3% nonzero, the user-user similarity matrix is 25% nonzero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(user_user_mats[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice - Evaluation\n",
    "\n",
    "670 valus are bigger then 0 while 6 (59-53) are bigger then 0.5.\n",
    "\n",
    "With the dice similarity one user has no similarity to any user. It's the same user as with the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(user_user_mats[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard - Evaluation\n",
    "\n",
    "670 valus bigger then 0 while no values are bigger then 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --> Intermediate summary:\n",
    "\n",
    "So far we've computed a sparse _user-food-matrix_ and 4 _user-user-matrices_ according to 4 different similarity functions ('Pearson Correlation', 'Cosine', 'Dice' and 'Jaccard').\n",
    "\n",
    "As these matrices represent user preferences for each user, we now reduce their dimensionality to 2 to plot their distribution on the 2D plane to inspect clusters / positioning of said user preference profiles.\n",
    "\n",
    "## Dim. Red. and Visualization\n",
    "\n",
    "Next, we (possibly) standardize the matrix values and apply PCA & t-SNE to see some first results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ANNOTATE_THRESH_NUM = 10\n",
    "\n",
    "\"\"\"Preprocessing\"\"\"\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# vals = df_user_food_ratings_matrix.values\n",
    "# x = StandardScaler().fit_transform(vals)\n",
    "\n",
    "# # pd.DataFrame(x)\n",
    "\n",
    "X_names = ['User-Food filled with 0s',\n",
    "           'User-Food filled with 2.5s']\n",
    "X_all = [df_user_item.values, \n",
    "         df_user_item.values]\n",
    "for user_user_mat in user_user_mats:\n",
    "    X_names.append('User-User ({})'.format(user_user_mat['metric']))\n",
    "    X_all.append(user_user_mat['df'].values)\n",
    "\n",
    "users_num_ratings = [np.sum([1 if r != 0 else 0 for r in user_ratings]) \n",
    "                     for user_ratings in df_user_item]\n",
    "# print(users_num_ratings)\n",
    "\n",
    "\"\"\"Dimensionality-reduction to 2D.\"\"\"\n",
    "\n",
    "nc, dfs = 2, []\n",
    "for i, X in enumerate(X_all):\n",
    "    dfs_X = []\n",
    "    pca = PCA(n_components=nc)\n",
    "    dfs_X.append({'title': 'PCA',\n",
    "                  'df': pd.DataFrame(pca.fit_transform(X), columns=['c1', 'c2'])})\n",
    "    tsne = TSNE(n_components=nc, perplexity=3)\n",
    "    dfs_X.append({'title': 't-SNE; perp=3',\n",
    "                  'df': pd.DataFrame(tsne.fit_transform(X), columns=['c1', 'c2'])})\n",
    "    tsne = TSNE(n_components=nc, perplexity=5)\n",
    "    dfs_X.append({'title': 't-SNE; perp=5',\n",
    "                  'df': pd.DataFrame(tsne.fit_transform(X), columns=['c1', 'c2'])})\n",
    "    tsne = TSNE(n_components=nc, perplexity=10)\n",
    "    dfs_X.append({'title': 't-SNE; perp=10',\n",
    "                  'df': pd.DataFrame(tsne.fit_transform(X), columns=['c1', 'c2'])})\n",
    "    dfs.append(dfs_X)\n",
    "\n",
    "\"\"\"Visualization.\"\"\"\n",
    "\n",
    "fig = plt.figure(figsize = (16,24))\n",
    "\n",
    "print('Number of components (dim.): {}'.format(nc))\n",
    "for i, dfs_X in enumerate(dfs):\n",
    "    print('Row {}: \"{}\"'.format(i+1, X_names[i]))\n",
    "    for j, df_type in enumerate(dfs_X):\n",
    "        ax = plt.subplot(len(dfs), len(dfs_X), i*len(dfs_X)+j+1)\n",
    "#         ax.set_xlabel('Component 1', fontsize = 15)\n",
    "#         ax.set_ylabel('Component 2', fontsize = 15)\n",
    "        ax.set_title(df_type['title'], fontsize = 25)\n",
    "        ax.grid()\n",
    "        scatter = ax.scatter(df_type['df'].loc[:, 'c1'], \n",
    "                             df_type['df'].loc[:, 'c2'],\n",
    "                             s = 50)\n",
    "#         for k,r in enumerate(users_num_ratings):\n",
    "#             if r > ANNOTATE_THRESH_NUM:\n",
    "#                 ax.annotate('User {0} ({1} ratings)'.format(k,r),\n",
    "#                             (df_type['df'].loc[k, 'c1'], df_type['df'].loc[k, 'c2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[...]\n",
    "\n",
    "We conclude that PCA scaptures points with a _representative enough_ number of ratings towards the outside compared to the cluster of few-rating-users, where there is a dendency towards different directions for different users.\n",
    "\n",
    "T-SNE on the other hand maximizes difference between different more-rating-users as it places them outside a cluster of similarly distributed few-rating-users. This intuitively makes sense at t-SNE aims to map distances in high-dim. space as closely as possible to distance in low-dim. space (frequently modeled as ith _springs_). This positions the only in a single dim. differing samples mostly uniformly in the inside and then stochastically distributes more complex positionde points outside."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
