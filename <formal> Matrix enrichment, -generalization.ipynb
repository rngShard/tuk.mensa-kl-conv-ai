{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from util.cloud_connection import bucket_connection\n",
    "from src.recommender import recommender\n",
    "\n",
    "\n",
    "recommender = recommender.Recommender()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich sparse matrix through SVD\n",
    "\n",
    "Motivated through [this blog post](http://nicolas-hug.com/blog/matrix_facto_4), we try enriching the sparse user-meal-matrix with Singular-Value-Decomposition and subsequent calculation of the missing entries with the generated user- / meal-vectors.\n",
    "The `surprise` framework will however (due to lacking docs) not utilized...\n",
    "\n",
    "From the `sklearn.cecomposition.TruncatedSVD` [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html):\n",
    "\n",
    "```\n",
    "\"Dimensionality reduction using truncated SVD (aka LSA).\n",
    "\n",
    "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.\n",
    "\n",
    "In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).\n",
    "\n",
    "This estimator supports two algorithms: a fast randomized SVD solver, and a “naive” algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SVD_K = 52    # only consider 10 most dominant \n",
    "\n",
    "df = recommender.user_similarity['cosine']\n",
    "\n",
    "svd = TruncatedSVD(SVD_K)\n",
    "df = pd.DataFrame(svd.fit_transform(df))\n",
    "\n",
    "vars_accum_sums = [svd.explained_variance_ratio_[:(i+1)].sum() for i in range(52)]\n",
    "plt.plot(vars_accum_sums)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above by the curve of accumulated variance explained through the first n most impactful vectors.\n",
    "\n",
    "We can see that the first 10 components only cover approx. 70% of total variance, while the first 20 components already cover approx. 90%. After 30 components, most of the variance is covered.\n",
    "\n",
    "This is an issue for visualization purposes, as the first 1-3 components do not hold a high enough expressability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_factors = 30  # number of factors\n",
    "alpha = .01  # learning rate\n",
    "n_epochs = 200  # number of iteration of the SGD procedure\n",
    "\n",
    "class sparseSVD:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.p = np.random.normal(0, .1, (data['n_users'], n_factors))\n",
    "        self.q = np.random.normal(0, .1, (data['n_items'], n_factors))\n",
    "\n",
    "    def SGD(self, data):\n",
    "        '''Learn the vectors p_u and q_i with SGD.\n",
    "           data is a dataset containing all ratings + some useful info (e.g. number\n",
    "           of items/users).\n",
    "        '''\n",
    "\n",
    "\n",
    "        # Randomly initialize the user and item factors.\n",
    "        p = np.random.normal(0, .1, (data['n_users'], n_factors))\n",
    "        q = np.random.normal(0, .1, (data['n_items'], n_factors))\n",
    "\n",
    "        # Optimization procedure\n",
    "        for _ in range(n_epochs):\n",
    "            for u, i, r_ui in data['all_ratings']:\n",
    "                err = r_ui - np.dot(p[u], q[i])\n",
    "                # Update vectors p_u and q_i\n",
    "                p[u] += alpha * err * q[i]\n",
    "                q[i] += alpha * err * p[u]\n",
    "        \n",
    "        self.p = p\n",
    "        self.q = q\n",
    "            \n",
    "    def estimate(self, u, i):\n",
    "        '''Estimate rating of user u for item i.'''\n",
    "        return np.dot(self.p[u], self.q[i])\n",
    "\n",
    "\n",
    "df = recommender.df_user_item\n",
    "data = {\n",
    "    'n_users': df.shape[0],\n",
    "    'n_items': df.shape[1],\n",
    "    'all_ratings': [(u,i, df.iloc[u,i]) for u in range(df.shape[0]) for i in range(df.shape[1]) if df.iloc[u,i] > 0]\n",
    "}\n",
    "\n",
    "svd = sparseSVD(data)\n",
    "svd.SGD(data)\n",
    "\n",
    "# svd.estimate(51,76)\n",
    "# data['all_ratings']\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     for i in row.size:\n",
    "\n",
    "\n",
    "new_pred = np.zeros(df.shape)\n",
    "for u in range(df.shape[0]):\n",
    "    for i in range(df.shape[1]):\n",
    "        new_pred[u,i] = svd.estimate(u,i)\n",
    "#         if df.iloc[u,i] > 0:\n",
    "#             print(\"{},{}:\\t actual={} \\t predicted={}\".format(u,i,df.iloc[u,i],svd.estimate(u,i)))\n",
    "df_new_pred = pd.DataFrame(new_pred, index=recommender.df_user_item.index, columns=recommender.df_user_item.columns)\n",
    "\n",
    "df_new_pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender.df_user_item.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 200 epochs, our SVD already nicely approximates the known ratings (e.g. user 1 rating 57 at 0.978 ~~ 1.0, user 4 rating 160 at 5.000124 ~~ 5.0). However, users with a low number of predominently low ratings (e.g. twice a 1.0) get only negative ratings assigned.\n",
    "\n",
    "We conclude that SVD learns user-wide and food-wide preferences which do not suffice to enrich sparse rows/columns of this matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
